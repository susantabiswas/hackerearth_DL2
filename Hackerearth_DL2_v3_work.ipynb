{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Hackerearth Deep Learning 2 Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Importing the Important stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import time\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two csv files: testing and training files.\n",
    "<br>We load both of them and look at the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and test data csv files\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>view_position</th>\n",
       "      <th>image_name</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0</td>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_0000.png</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1</td>\n",
       "      <td>57</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_0001.png</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_10</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_00010.png</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_1000</td>\n",
       "      <td>64</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_0001000.png</td>\n",
       "      <td>class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_10000</td>\n",
       "      <td>33</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>scan_00010000.png</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  age gender  view_position         image_name detected\n",
       "0      id_0   45      M              0      scan_0000.png  class_3\n",
       "1      id_1   57      F              0      scan_0001.png  class_3\n",
       "2     id_10   58      M              0     scan_00010.png  class_3\n",
       "3   id_1000   64      M              0   scan_0001000.png  class_6\n",
       "4  id_10000   33      M              1  scan_00010000.png  class_3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the structure of contents of the training csv file\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>view_position</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_100</td>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_000100.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_10002</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_00010002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_10005</td>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>scan_00010005.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_10008</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>scan_00010008.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_10009</td>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>scan_00010009.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  age gender  view_position         image_name\n",
       "0    id_100   47      M              0    scan_000100.png\n",
       "1  id_10002   28      M              0  scan_00010002.png\n",
       "2  id_10005   56      F              0  scan_00010005.png\n",
       "3  id_10008   41      M              1  scan_00010008.png\n",
       "4  id_10009   56      F              1  scan_00010009.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the structure of contents of the test csv file\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>view_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18577.000000</td>\n",
       "      <td>18577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.972601</td>\n",
       "      <td>0.573343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.741076</td>\n",
       "      <td>0.494605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>414.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  view_position\n",
       "count  18577.000000   18577.000000\n",
       "mean      47.972601       0.573343\n",
       "std       16.741076       0.494605\n",
       "min        2.000000       0.000000\n",
       "25%       36.000000       0.000000\n",
       "50%       50.000000       1.000000\n",
       "75%       60.000000       1.000000\n",
       "max      414.000000       1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the stats related to training data\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>view_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12386.000000</td>\n",
       "      <td>12386.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.028984</td>\n",
       "      <td>0.579445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.660680</td>\n",
       "      <td>0.493668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  view_position\n",
       "count  12386.000000   12386.000000\n",
       "mean      48.028984       0.579445\n",
       "std       16.660680       0.493668\n",
       "min        1.000000       0.000000\n",
       "25%       36.000000       0.000000\n",
       "50%       50.000000       1.000000\n",
       "75%       60.000000       1.000000\n",
       "max      152.000000       1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see stats related to test data\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: 14\n"
     ]
    }
   ],
   "source": [
    "print('Unique classes: {}'.format(train_data['detected'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 18577 training and 12386 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuAAAAGaCAYAAABOj/YzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JABXJIkSWro/wM3ySe/LhEeLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e0fe71d978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram for the output classes\n",
    "detected_counts = train_data.detected.value_counts()\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(detected_counts.index, detected_counts.values, alpha = 0.9)\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Detected classes', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For resizing the images in a given resolution and saving on th disk for faster loading later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a folder named dataset.Inside it make a folder **train128** to store the resized pictures.\n",
    "<br>The Structure will look like this : **dataset\\train128**.\n",
    "<br>Place the **train_** and **test_** folders in outside dataset folder. That is the structure will be: **\\train_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(source, dest, img_size):\n",
    "    for img_path in tqdm(train_data['image_name'].values):\n",
    "        img = cv2.imread(source + img_path, 0)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        cv2.imwrite(dest + img_path, img)\n",
    "    del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the source folder and destination folder\n",
    "source = 'train_\\\\'\n",
    "dest = 'dataset\\\\train128\n",
    "img_size = 64\n",
    "save_file(source, dest, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make output training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = train_data['detected'].tolist()\n",
    "# make a mapping of different classes with integer values\n",
    "class_dict = {name:counter+1 for counter, name in enumerate(set(class_list))}\n",
    "# create output labels\n",
    "y_train = [class_dict[k] for k in class_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18577, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the output labels to one hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0, train_data.shape[0]-1)\n",
    "img = cv2.imread(TRAIN_PATH + train_data['image_name'].iloc[index])\n",
    "plt.imshow(img)\n",
    "print(\"Image dimension:{}\\nImage Resolution: {} X {}\".format(img.shape, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model:   VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Transfer learning so , we will use VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 18577/18577 [03:28<00:00, 89.08it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'dataset\\\\train128\\\\' \n",
    "X_train=[]\n",
    "\n",
    "for i in tqdm(range(len(train_data))): #, target_size=(128,128)\n",
    "    temp_img=image.load_img(TRAIN_PATH+train_data['image_name'][i])\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    X_train.append(temp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18577, 128, 128, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting train images to array \n",
    "X_train = np.array(X_train) \n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying mean subtraction processing\n",
    "X_train = preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.array(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18577, 4, 4, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the output from the pretrained model just before its FC layer and use it for our Neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "np.save('vgg16_tr_full_128.npy', features_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U> Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the keras libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import applications\n",
    "from keras import optimizers \n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.io_utils import HDF5Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=3, monitor='acc', verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model checkpoint\n",
    "file_path=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Score: 0.30561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(log_dir='./logvgg', histogram_freq=0, batch_size=32, \n",
    "                           write_graph=True, write_grads=False, write_images=False, \n",
    "                           embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the features\n",
    "X_train = np.load('vgg16_tr_full_128.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18577, 4, 4, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18577, 8192)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape the numpy matrix such that can be feed as input for the FC Neural Network\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train.shape[1], use_bias=False, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, use_bias=False, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "         \n",
    "model.add(Dense(256, use_bias=False,  kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(units = 15, activation='softmax', use_bias=False, kernel_initializer='glorot_uniform'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1024)              8388608   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              1048576   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 15)                15360     \n",
      "=================================================================\n",
      "Total params: 9,460,736\n",
      "Trainable params: 9,456,640\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now start Training\n",
    "We have designed the CNN architecture ,now we will train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model weights\n",
    "try:\n",
    "    model.load_weights(file_path) # load the best model\n",
    "except:\n",
    "    pass # no file found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb023409b94b569f02c74954bd93fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3926b60f6d8a426ba79a396a524370eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 00001: loss did not improve\n",
      " - 21s - loss: 0.2490 - acc: 0.9176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e917c4c8ce5c4708ae3323d06ac4d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 00002: loss did not improve\n",
      " - 21s - loss: 0.2452 - acc: 0.9151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626d6622730427e9721e13e93b0406b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Epoch 00003: loss did not improve\n",
      " - 21s - loss: 0.2083 - acc: 0.9282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ad380213f24c808ebd49c4453c1432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Epoch 00004: loss did not improve\n",
      " - 21s - loss: 0.2115 - acc: 0.9263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079de586f8564a6bae5bea804fcd7a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Epoch 00005: loss did not improve\n",
      " - 21s - loss: 0.2018 - acc: 0.9317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a967316ea03b472b9ee0ba230abc7980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Epoch 00006: loss did not improve\n",
      " - 21s - loss: 0.1852 - acc: 0.9363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b554c9ad939244d888d72ada48592478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Epoch 00007: loss did not improve\n",
      " - 21s - loss: 0.1657 - acc: 0.9428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb949795a7641ca9df26e7e9f832094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Epoch 00008: loss did not improve\n",
      " - 21s - loss: 0.1544 - acc: 0.9466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb7635f1a794022a41c8f76f3da1d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Epoch 00009: loss did not improve\n",
      " - 21s - loss: 0.1760 - acc: 0.9392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433b52673dbb404380d36f0869a43635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Epoch 00010: loss did not improve\n",
      " - 21s - loss: 0.1648 - acc: 0.9430\n",
      "3.5379504362742105 minutes\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "history = model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, verbose=2, \n",
    "                    callbacks=[TQDMNotebookCallback(leave_inner=False), checkpoint, tensor_board, early_stop],\n",
    "                    validation_split=0.0,  shuffle=True, initial_epoch=0)\n",
    "t2 = time.time()\n",
    "\n",
    "elapsed = t2 - t1\n",
    "print(str(elapsed/60) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('vgg_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <u>Loading the Test dataset for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 12386/12386 [01:47<00:00, 115.33it/s]\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = 'dataset\\\\test128\\\\'\n",
    "X_test=[]\n",
    "\n",
    "for i in tqdm(range(len(test_data))): #, target_size=(128,128)\n",
    "    temp_img=image.load_img(TEST_PATH+test_data['image_name'][i])\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    X_test.append(temp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12386, 128, 128, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting train images to array \n",
    "X_test = np.array(X_test) \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying mean subtraction processing\n",
    "X_test = preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "model = VGG16(weights='imagenet', include_top=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = np.array(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "np.save('vgg16_te_full_128.npy', features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model weights\n",
    "try:\n",
    "    model.load_weights(file_path) # load the best model\n",
    "except:\n",
    "    pass # no file found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# identical to the previous one\n",
    "model = load_model('vgg_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved test input \n",
    "X_test = np.load('vgg16_te_full_128.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12386, 4, 4, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12386, 8192)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to suit the FC layer\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12386,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a reverse mapping dict for labelling\n",
    "rev_class_dict = {counter: name for name, counter in class_dict.items()}\n",
    "pred_labels = [rev_class_dict[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_100</td>\n",
       "      <td>class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_10002</td>\n",
       "      <td>class_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_10005</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_10008</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_10009</td>\n",
       "      <td>class_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  detected\n",
       "0    id_100   class_6\n",
       "1  id_10002  class_14\n",
       "2  id_10005   class_3\n",
       "3  id_10008   class_3\n",
       "4  id_10009   class_6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame({'row_id':test_data.row_id, 'detected':pred_labels})\n",
    "sub = sub[['row_id', 'detected']]\n",
    "filename = 'submission_vgg.csv'\n",
    "sub.to_csv(filename, index=False) \n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
